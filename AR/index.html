<!DOCTYPE html>
<html lang="en">
  <head>
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script
      id="MathJax-script"
      async
      src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"
    ></script>
    <meta charset="UTF-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Fun with Filters and Frequencies!</title>
    <style>
      body {
        font-family: Arial, sans-serif;
        margin: 0;
        padding: 0;
        background-color: #f4f4f4;
      }
      header {
        background-color: #333;
        color: white;
        padding: 10px 20px;
        text-align: center;
      }
      .container {
        max-width: 1000px;
        margin: 20px auto;
        padding: 20px;
        background-color: white;
        box-shadow: 0 0 10px rgba(0, 0, 0, 0.1);
      }
      h2 {
        color: #333;
      }
      p {
        line-height: 1.6;
      }
      .image-section {
        display: flex;
        justify-content: center;
        flex-wrap: wrap;
        gap: 20px;
        margin-top: 20px;
        width: 100%;
        height: auto;
      }
      .image-card {
        flex: 1;
        min-width: 100px;
        max-width: 1000px;
        text-align: center;
      }
      .image-card img {
        width: 100%;
        max-width: 500px;
        height: auto;
        border: 1px solid #ddd;
        border-radius: 5px;
      }
      .image-card p {
        margin-top: 10px;
      }
      .row-images {
        display: flex;
        flex-direction: row;
        justify-content: space-between;
        gap: 10px;
        flex-wrap: nowrap;
        width: 100%;
        overflow-x: auto;
        }

        .row-images .image-card {
        flex: 1;
        text-align: center;
        }

        .row-images .image-card img {
        width: 100px; /* Set smaller width only for these specific images */
        height: auto;
        border: 1px solid #ddd;
        border-radius: 5px;
        }
      .two-column {
        display: flex;
        flex-wrap: wrap; /* Ensures responsiveness */
        gap: 20px; /* Adds some space between the columns */
        margin-top: 20px;
      }

      .text-column {
        flex: 1;
        min-width: 200px;
      }

      .image-column {
        flex: 1;
        min-width: 200px;
        text-align: center;
      }

      .image-column img {
        max-width: 100%;
        height: auto;
        border: 1px solid #ddd;
        border-radius: 5px;
      }
      .video-container {
      text-align: center;
    }
    img {
      max-width: 100%;
      height: auto;
      border: 2px solid #ccc;
      border-radius: 8px;
    }
    p {
      margin: 10px 0 0;
      font-size: 1rem;
      color: #333;
    }
    </style>
  </head>
  <body>
    <header>
      <h1>Final Project(s)</h1>
    </header>
    <div class="container">
      <section>
        <h2>Augmented Reality Project Overview</h2>
        <p>In this project I insert a synthetic object into a video I captured which is a basic fundamental of AR. The idea is to use 2D points from the video and 3D coordinates that are known to calibrate the camera for each frame and use projection matrix to project the 3D coordinates of your synthetic object into your 2D frames in the video.</p>
      </section>
      <section>
        <h2>Input Video</h2>
        <p>
          I drew a regular grid pattern on a box so that I could easily annotate the points and measure their real 3D coordinates. I then took a simply video in which I pan around the box.
        </p>
        <div class="video-container">
            <img src="media/input.gif" alt="Input GIF">
            <p>Input Video</p>
        </div>
      </section>
      <section>
        <h2>Keypoints with known 3D world coordinates</h2>
          <p>
            The first step is to extract the first frame from the video and annotate some keypoints for that image, I chose to annotate 32 keypoints in the end.
            Next, you need to define 3D cooridinates for those same points. Since I drew a grid with 2 inch spacing on my box, I was easily able to define 3D coordinates for each of my 2D keypoints.
            Now that we have 2D keypoints for the first frame we need to propogate those for each frame in the video. <br/>
            To do this I used <b>cv2.calcOpticalFlowPyrLK</b> which is Optical Flow with the Lucas-Kanade method for feature point tracking. It computes the motion of a list of points between two frames by using local gradients of pixel intensities to get the displacement. There were some points that it struggled to properly track which gave weird results, so I simply removed those points from my projection matrix calculation.
          </p>
        <div class="video-container">
        <img src="media/output_video_with_tracking.gif" alt="Tracking GIF">
        <p>Tracking all points</p>
        </div>
        <div class="video-container">
        <img src="media/output_video_with_tracking3.gif" alt="Tracking GIF">
        <p>Tracking removed bad points</p>
    </div>
      </section>
        <section>
            <h2>Calibrating the Camera</h2>
    <p>
        Now that we have the 2D points for each frame in the video, we can calibrate the camera by finding a projection matrix that maps the 3D coordinates of points in the world to their corresponding 2D image coordinates. The process involves solving for the camera's projection matrix using the least squares method, which minimizes the error between the predicted and actual 2D points.
    </p>
    <p>
        To do this, we need to solve the equation:
        <br>
        <strong>p = P * X</strong>
        <br>
        where <strong>p</strong> is the 2D point in homogeneous coordinates (x, y, 1), <strong>P</strong> is the 3x4 camera projection matrix, and <strong>X</strong> is the 3D point in homogeneous coordinates (X, Y, Z, 1). The goal is to find <strong>P</strong> that best maps the 3D points to the corresponding 2D points across all frames.
    </p>
    <p>
        The matrix equation can be rewritten for multiple points as:
        <br>
        <strong>[p₁] [x₁ y₁ z₁ 1] </strong><br>
        <strong>[p₂] [x₂ y₂ z₂ 1]</strong><br>
        <strong>[p₃] [x₃ y₃ z₃ 1]</strong><br>
        <strong>...   ...   ...   ...</strong><br>
        <strong> [pₖ] [xₖ yₖ zₖ 1]</strong><br>
        </p>
        <p>
        We can solve this system of equations using a least squares approach, where we minimize the sum of squared differences between the actual and predicted 2D points. The resulting matrix <strong>P</strong> consists of the intrinsic and extrinsic parameters of the camera, which include the camera's focal length, principal point, and orientation in space.
        </p>
    </p>
    </section>
        <section>
            <h2>Projecting a cube in the Scene</h2>
            <p>Now that we have a projection matrix for each frame, we just use it to project the 3D coordinates of our cube onto that 2D frame and then draw the cube onto the frame. </p>
            <div class="video-container">
                <img src="media/output_video_with_cube3.gif" alt="Output GIF">
                <p>Output Video with Cube</p>
            </div>
        </section>
    </div>
    <div class="container">
        <section>
            <h2>High Dynamic Range Imaging Project Overview</h2>
            <p>Modern cameras often struggle to capture the full dynamic range of real-world scenes, resulting in images that may be partially underexposed or overexposed. To address this, both photographers and researchers commonly merge data from multiple exposures of the same scene. In this project, you will develop software that automatically combines these multiple exposures into a single high dynamic range (HDR) radiance map and applies tone mapping to convert this map into a displayable image.
                This project is based on the <a href="https://www.pauldebevec.com/Research/HDR/debevec-siggraph97.pdf"> Debevec and Malik 1997</a> paper.
            </p>
        </section>
        <section>
            <h2>Input Images</h2>
            <p>We start with images taken at various exposures of a stationary scene, an example shown below.</p>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/1_25.jpg" alt="" />
                    <p>1/25 sec</p>
                </div>
                <div class="image-card">
                  <img src="media/1_4.jpg" alt="" />
                  <p>1/4 sec</p>
                </div>
                <div class="image-card">
                    <img src="media/3_1.jpg" alt="" />
                    <p>3 sec</p>
                </div>
                <div class="image-card">
                    <img src="media/17_1.jpg" alt="" />
                    <p>17 sec</p>
                </div>
            </section>
        </section>
        <section>
            <h2>Radiance Map Construction</h2>
                <p>
                    The observed pixel value, <i>Z<sub>ij</sub></i>, for pixel <i>i</i> in image <i>j</i> is a function of the unknown scene radiance and the known exposure duration: 
                    <i>Z<sub>ij</sub> = f(E<sub>i</sub> Δt<sub>j</sub>)</i>. Here, <i>E<sub>i</sub></i> represents the unknown scene radiance at pixel <i>i</i>, and 
                    <i>E<sub>i</sub> Δt<sub>j</sub></i> denotes the exposure at that pixel. The function <i>f</i> represents the camera's pixel response curve, which is often non-linear.
                </p>
                <p>
                    Instead of solving directly for <i>f</i>, we solve for <i>g = ln(f<sup>-1</sup>)</i>, which maps pixel values (0–255) to the logarithm of exposure values. The relationship can be expressed as: 
                    <i>g(Z<sub>ij</sub>) = ln(E<sub>i</sub>) + ln(Δt<sub>j</sub>)</i> (Equation 2, Debevec). Since the scene is static, while we may not know the absolute radiance value <i>E<sub>i</sub></i>, it remains constant across all exposures of the same pixel.
                </p>
                <p>
                    To make the results more robust:
                    <ul>
                    <li>
                        <b>Ensure smoothness:</b> A penalty is added to the linear system for large second derivatives of <i>g</i>. The second derivative is approximated as 
                        <i>g(x-1) + g(x+1) - 2g(x)</i>, applied across all pixel values except at the boundaries (<i>g(0)</i> and <i>g(255)</i>).
                    </li>
                    <li>
                        <b>Use pixel weighting:</b> Each exposure contributes reliable information only for well-exposed pixels. Weighting is applied according to 
                        Equation 6 in Debevec, using a triangle function that peaks at <i>Z = 127.5</i> and is zero at <i>Z = 0</i> and <i>Z = 255</i>.
                    </li>
                    </ul>
                </p>
                <section class="image-section">
                    <div class="image-card">
                        <img src="media/garage_g_plot.png" alt="" />
                        <p></p>
                    </div>
                </section>
                <p>
                    Once <i>g</i> is solved, radiance values can be calculated using: <i>ln(E<sub>i</sub>) = g(Z<sub>ij</sub>) - ln(Δt<sub>j</sub>)</i> (Equation 5, Debevec).
                </p>
                <section class="image-section">
                    <div class="image-card">
                        <img src="media/garage_hdr_radiance_map_mean.png" alt="" />
                        <p>Mean radiance map</p>
                    </div>
                    <div class="image-card">
                        <img src="media/garage_hdr_radiance_map.png" alt="" />
                        <p>Per channel radiance map</p>
                    </div>
                </section>
        </section>
        <section>
            <h2>Tone Mapping</h2>
                <p>
                    Once the radiance map is generated, tone mapping is used to compress its dynamic range for display. 
                    We implemented both global and local tone mapping techniques:
                </p>
                <h3>Global Tone Mapping</h3>
                <p>
                    The global method applies a simple compression formula:
                    <code>E_display = E_world / (1 + E_world)</code>. This effectively reduces high-intensity values 
                    while maintaining overall contrast. The result is normalized to the [0, 1] range for display.
                </p>
                <h3>Local Tone Mapping</h3>
                <p>
                    For a more sophisticated approach, we implemented a simplified version of Durand 2002:
                </p>
                <ol>
                    <li>Compute intensity as the average of the RGB channels.</li>
                    <li>Extract chrominance as <code>R/I, G/I, B/I</code>.</li>
                    <li>Transform intensity to log space and filter it using a bilateral filter to obtain a base layer.</li>
                    <li>Calculate a detail layer as the difference between the log intensity and the base layer.</li>
                    <li>Apply scaling and offset to the base layer to control dynamic range compression.</li>
                    <li>Reconstruct the intensity by combining the scaled base and detail layers.</li>
                    <li>Restore the color by multiplying the intensity with the chrominance and apply gamma compression for display.</li>
                </ol>
                <p>
                    The combination of these methods allows for effective visualization of high dynamic range images 
                    while preserving local details and contrast.
                </p>
                <section class="image-section">
                    <div class="image-card">
                        <img src="media/garage_global_scale.png" alt="" />
                        <p>Global Scaling</p>
                    </div>
                    <div class="image-card">
                        <img src="media/garage_global_simple.png" alt="" />
                        <p>Global Simple</p>
                    </div>
                    <div class="image-card">
                        <img src="media/garage_durand.png" alt="" />
                        <p>Durand</p>
                    </div>
                </section>
                <p>
                    Although the difference between the three tone mapping algorithms is not super strong, you can see that the simple scale fails to reduce contrast and some details are lost in the dark. The global simple method is an improvement but it still has some saturation in the darkest and lightest parts of the image. The Durand however remedies this and overall looks quite good.
                </p>
        </section>
        <section>
            <h2>All HDR Images</h2>
            <p>Below are the results for the arch, bonsai, chapel, gatrage, garden, house, mug, and window for both global simple and Durand tone mapping.</p>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/garage_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/garage_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/house_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/house_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/chapel_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/chapel_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/arch_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/arch_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/garden_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/garden_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/window_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/window_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/mug_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/mug_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
            <section class="image-section">
                <div class="image-card">
                    <img src="media/bonsai_global_simple.png" alt="" />
                    <p>Global Simple</p>
                </div>
                <div class="image-card">
                    <img src="media/bonsai_durand.png" alt="" />
                    <p>Durand</p>
                </div>
            </section>
        </section>
    </div>
  </body>
</html>
